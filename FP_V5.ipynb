{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as ltb\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "from scipy.stats import linregress\n",
    "import scipy.linalg\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error, r2_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import joblib\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_seq_items', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'C:/Users/Edward/Documents/Appleolga/Final_pro/model2/'\n",
    "# dir_path = 'C:/Users/Пользователь/Desktop/BDL/Final_projects/model2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING INITIAL TRAIN DATA\n",
    "data_traff_train = pd.read_sas(dir_path +'train/hash_school_dpi_model_traff.sas7bdat')\n",
    "data_train = pd.read_sas(dir_path + 'train/hash_school_dpi_model_fe.sas7bdat')\n",
    "\n",
    "#READING INITIAL TEST DATA\n",
    "data_traff_test = pd.read_sas(dir_path + 'test/hash_school_dpi_model_traff_test.sas7bdat')\n",
    "data_test = pd.read_sas(dir_path + 'test/hash_school_dpi_model_fe_test.sas7bdat')\n",
    "data_test2 = pd.read_sas(dir_path + 'test/hash_school_dpi_model_test.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DUPLICATES CLEANING AND MERGING INITIAL DATA FILES\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_traff_train.shape)\n",
    "data_traff_train = data_traff_train.drop_duplicates()\n",
    "data_train = data_train.drop_duplicates()\n",
    "print(data_train.shape)\n",
    "print(data_traff_train.shape)\n",
    "\n",
    "data_train = data_train.merge(data_traff_train, on = 'abon_id')\n",
    "del data_traff_train\n",
    "\n",
    "print(data_test.shape)\n",
    "print(data_test2.shape)\n",
    "print(data_traff_test.shape)\n",
    "data_test = data_test.drop_duplicates()\n",
    "data_test2 = data_test2.drop_duplicates()\n",
    "data_traff_test = data_traff_test.drop_duplicates()\n",
    "print(data_test.shape)\n",
    "print(data_test2.shape)\n",
    "print(data_traff_test.shape)\n",
    "\n",
    "data_test = data_test.merge(data_test2, on = 'abon_id').merge(data_traff_test, on = 'abon_id')\n",
    "del data_test2\n",
    "del data_traff_test\n",
    "\n",
    "data_train = data_train.astype('float32')\n",
    "data_test = data_test.astype('float32')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # WRITING MERGED TRAIN AND TEST DATA TO PARQUET\n",
    "# # data_train.to_parquet((dir_path + 'train/data_train.parquet.gzip'),compression='gzip')\n",
    "# # data_test.to_parquet((dir_path + 'test/data_test.parquet.gzip'),compression='gzip')\n",
    "\n",
    "# # # READING INITIAL MERGED TRAIN AND TEST DATA FROM PARQUET\n",
    "data_train = pd.read_parquet(dir_path + 'train/data_train.parquet.gzip')\n",
    "data_test = pd.read_parquet(dir_path + 'test/data_test.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKING TRAFFIC VARIANCE\n",
    "\n",
    "traffic_cols = ['traff_m5', 'traff_m4', 'traff_m3','traff_m2','traff_m1']\n",
    "\n",
    "traff_var = pd.DataFrame(data = data_train[traffic_cols].var(), columns = ['traff_var'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>traff_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>traff_m5</th>\n",
       "      <td>9.908881e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traff_m4</th>\n",
       "      <td>7.943503e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traff_m3</th>\n",
       "      <td>1.277479e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traff_m2</th>\n",
       "      <td>1.252335e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traff_m1</th>\n",
       "      <td>1.433794e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             traff_var\n",
       "traff_m5  9.908881e+01\n",
       "traff_m4  7.943503e+07\n",
       "traff_m3  1.277479e+02\n",
       "traff_m2  1.252335e+02\n",
       "traff_m1  1.433794e+02"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traff_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_outliers(df, outlier_coeff = 1.5): \n",
    "    \n",
    "    outliers_mask = df.copy()\n",
    "    counter = 1\n",
    "    for col in df.columns:\n",
    "        \n",
    "        p75, p25 = df[col].quantile(0.75), df[col].quantile(0.25)\n",
    "        IQR_col = p75 - p25\n",
    "        upper, lower = p75 + outlier_coeff*IQR_col, p25 - outlier_coeff*IQR_col\n",
    "        outliers_mask[col] = df[col].apply(lambda x: True if(x > upper or x < lower) else False)\n",
    "        \n",
    "        if (counter%100 == 0):\n",
    "            print(counter)\n",
    "        counter += 1\n",
    "        \n",
    "    return outliers_mask  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO FIND CATEGORICAL COLUMNS\n",
    "\n",
    "def find_cat_cols (df):\n",
    "    \n",
    "    cat_cols_list = pd.DataFrame(columns = ['col_name', 'if_cat'])\n",
    "    \n",
    "    df_length = df.shape[0]\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        value_count_info = df[col].value_counts().sort_values(ascending = False)\n",
    "        \n",
    "        if (np.array_equal(value_count_info.index, [1,0]) or np.array_equal(value_count_info.index,[0,1])):\n",
    "            if_cat = 1 \n",
    "        else: \n",
    "            if_cat = 0\n",
    "            \n",
    "        cat_cols_list = cat_cols_list.append({'col_name': col, 'if_cat' : if_cat}, ignore_index=True)\n",
    "        \n",
    "    return cat_cols_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "traff_outliers_mask = mark_outliers(traff_var)\n",
    "if traff_outliers_mask[traff_outliers_mask['traff_var']].values[0][0]:\n",
    "    m_to_exclude = traff_outliers_mask[traff_outliers_mask['traff_var']].index.values[0]\n",
    "    traffic_cols.remove(m_to_exclude)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CALCULATION OF TRAFFIC VARIANCE AND TRAFFIC COLS LIN.REGR. SLOPE FOR EACH OBSERVATION\n",
    "\n",
    "data_train = data_train.assign(var = data_train[traffic_cols].apply(lambda x: x.var(), axis=1))\n",
    "data_train = data_train.assign(var_quant = pd.qcut(data_train['var'], 10,labels = False))\n",
    "\n",
    "data_test = data_test.assign(var = data_test[traffic_cols].apply(lambda x: x.var(), axis=1))\n",
    "data_test = data_test.assign(var_quant = pd.qcut(data_test['var'], 10,labels = False))\n",
    "\n",
    "enum = np.arange(1, len(traffic_cols)+1, 1)\n",
    "\n",
    "data_train = data_train.assign(slope_traff = data_train.apply(lambda x: linregress(enum, x[traffic_cols]).slope, axis=1))\n",
    "\n",
    "data_test = data_test.assign(slope_traff = data_test.apply(lambda x: linregress(enum, x[traffic_cols]).slope, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # WRITING TRAIN AND TEST DATA WITH VARIANCE, QUANTILES AND SLOPE TO PARQUET\n",
    "\n",
    "# data_train.to_parquet((dir_path + 'train/data_train_with_var.parquet.gzip'),compression='gzip')\n",
    "# data_test.to_parquet((dir_path + 'test/data_test_with_var.parquet.gzip'),compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # READING TRAIN AND TEST DATA WITH VARIANCE QUANTILES AND SLOPE FROM PARQUET\n",
    "\n",
    "data_train = pd.read_parquet(dir_path + 'train/data_train_with_var.parquet.gzip')\n",
    "data_test = pd.read_parquet(dir_path + 'test/data_test_with_var.parquet.gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "879\n"
     ]
    }
   ],
   "source": [
    "cat_num_col_list = find_cat_cols(data_train)\n",
    "cat_col_list = list(cat_num_col_list[cat_num_col_list['if_cat'] == 1]['col_name'].values)\n",
    "num_col_list = list(cat_num_col_list[cat_num_col_list['if_cat'] == 0]['col_name'].values)\n",
    "print(len(cat_col_list))\n",
    "print(len(num_col_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_num = data_train[num_col_list]\n",
    "data_train_num_outliers_mask = mark_outliers(data_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # READING TRAIN AND TEST DATA NUMERICAL FEAUTURES ONLY WITH VARIANCE QUANTILES AND SLOPE FROM PARQUET\n",
    "\n",
    "data_train_num = pd.read_parquet(dir_path + 'model2_parquet/data_train_num.parquet.gzip')\n",
    "data_test_num = pd.read_parquet(dir_path + 'model2_parquet/data_test_num.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO MARK UNIVALUE COLUMNS, OUTLIERS, MISSING VALUES\n",
    "\n",
    "def df_info (df, outliers_mask_df):\n",
    "    \n",
    "    df_info = pd.DataFrame(columns=['col', '%of_fr_univalue', '%of_unfr_univalue','freq_value', '%outliers', '%nans'])\n",
    "    \n",
    "    df_length = df.shape[0]\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        value_count_info = df[col].value_counts().sort_values(ascending = False)\n",
    "        \n",
    "        freq_value = value_count_info.index[0]\n",
    "        freq_value_count = value_count_info.iloc[0]/df_length\n",
    "        #print(freq_value_count)\n",
    "        try:\n",
    "            unfreq_value_count = value_count_info.iloc[1]/df_length\n",
    "        except:\n",
    "            unfreq_value_count = np.nan\n",
    "        #print(unfreq_value_count)\n",
    "        perc_outl = outliers_mask_df[col].sum()/df_length\n",
    "        perc_nans = df[col].isnull().sum()/df_length\n",
    "        \n",
    "        #print(freq_count)\n",
    "        df_info = df_info.append({'col': col, \n",
    "                                  '%of_fr_univalue': freq_value_count,\n",
    "                                  '%of_unfr_univalue': unfreq_value_count,\n",
    "                                  '%outliers': perc_outl,\n",
    "                                  '%nans': perc_nans,\n",
    "                                  'freq_value': freq_value}, \n",
    "                                 ignore_index=True)\n",
    "        \n",
    "    return df_info.sort_values(by = ['%of_fr_univalue', '%outliers','%nans'], ascending=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_num_info = df_info(data_train_num, data_train_num_outliers_mask)\n",
    "del data_train_num_outliers_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_num_corr = data_train_num.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # WRITING AND READING DATA_TRAIN_INFO DF TO PARQUET\n",
    "\n",
    "# # data_train_num_info.to_parquet((dir_path + 'data_train_num_info.parquet.gzip'),compression='gzip')\n",
    "\n",
    "data_train_num_info = pd.read_parquet(dir_path +'data_train_num_info.parquet.gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # WRITING AND READING DATA_TRAIN TARGET CORRELATION DF TO PARQUET\n",
    "\n",
    "# # data_train_num_corr.to_parquet((dir_path + 'data_train_num_corr.parquet.gzip'),compression='gzip')\n",
    "\n",
    "data_train_num_corr = pd.read_parquet(dir_path +'data_train_num_corr.parquet.gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDING TARGET CORR TO THE INFO DF\n",
    "data_train_num_info = data_train_num_info.merge(data_train_num_corr['target'], left_on='col', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_train_num_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIGHLIGHT DESIRED PERCENTILE IN A SERIES\n",
    "\n",
    "def highlight_perc(s, q):\n",
    "    \n",
    "    is_max = s >= s.quantile(q)\n",
    "    \n",
    "    return ['background-color: salmon' if v else '' for v in is_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRINTING INFO DF WITH CONDITIONAL FORMATTING\n",
    "data_train_num_info_style = data_train_num_info.sort_values(by = '%nans', \n",
    "                                                            ascending=False).\\\n",
    "                                                style.apply(highlight_perc,\n",
    "                                                            subset=['%outliers', '%nans'],\n",
    "                                                            **{'q': 0.75}).\\\n",
    "                                                format({'%of_fr_univalue': \"{:.2%}\",\n",
    "                                                        '%of_unfr_univalue': \"{:.2%}\",\n",
    "                                                        '%outliers': \"{:.2%}\",\n",
    "                                                        '%nans': \"{:.2%}\",\n",
    "                                                        'target': \"{:.2f}\"})\n",
    "data_train_num_info_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('low target corr features number:')\n",
    "print(data_train_num_info[data_train_num_info['target'].abs() < 0.2].shape[0])\n",
    "\n",
    "\n",
    "#PRINTING INFO DF FOR LOW TARGET CORR FEATURES WITH CONDITIONAL FORMATTING\n",
    "\n",
    "data_train_num_info_style = data_train_num_info[data_train_num_info['target'].abs() < 0.2].\\\n",
    "                            sort_values(by = 'target', ascending=False).\\\n",
    "                            style.apply(highlight_perc,\n",
    "                                        subset=['%outliers', '%nans'],\n",
    "                                        **{'q': 0.75}).\\\n",
    "                                        format({'%of_fr_univalue': \"{:.2%}\",\n",
    "                                                '%of_unfr_univalue': \"{:.2%}\",\n",
    "                                                '%outliers': \"{:.2%}\",\n",
    "                                                '%nans': \"{:.2%}\",\n",
    "                                                'target': \"{:.2f}\"})\n",
    "data_train_num_info_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('high target corr features number:')\n",
    "print(data_train_num_info[data_train_num_info['target'].abs() >= 0.2].shape[0])\n",
    "\n",
    "#PRINTING INFO DF FOR HIGH TARGET CORR FEATURES WITH CONDITIONAL FORMATTING\n",
    "\n",
    "data_train_num_info_style = data_train_num_info[data_train_num_info['target'].abs() >= 0.2].\\\n",
    "                            sort_values(by = '%nans', ascending=False).\\\n",
    "                            style.apply(highlight_perc,\n",
    "                                        subset=['%outliers', '%nans'],\n",
    "                                        **{'q': 0.75}).\\\n",
    "                                        format({'%of_fr_univalue': \"{:.2%}\",\n",
    "                                                '%of_unfr_univalue': \"{:.2%}\",\n",
    "                                                '%outliers': \"{:.2%}\",\n",
    "                                                '%nans': \"{:.2%}\",\n",
    "                                                'target': \"{:.2f}\"})\n",
    "data_train_num_info_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHOOSING THE FEATURES WITH AT LEAST 0.2 CORRELATION COEFF WITH THE TARGET\n",
    "# AND LESS THAN 60% OF MISSINGS\n",
    "\n",
    "num_cols_to_use = data_train_num_info[(data_train_num_info['target'].abs() >= 0.2) &\\\n",
    "                                      (data_train_num_info['target'].abs() < 1) &\\\n",
    "                                      (data_train_num_info['%nans'] < 0.6)]['col'].to_list()\n",
    "len(num_cols_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGING INFO DF WITH FEAUTURES DESCRIPTION AND CHOOSING MAX TARGET CORRELATED STAT METRIC FEAUTURES\n",
    "data_descr = pd.read_csv((dir_path + 'data_descr.csv'), names=['feature', 'descr'])\n",
    "\n",
    "data_train_num_info = pd.merge(data_train_num_info, data_descr, how='left', left_on='col', right_on='feature')\n",
    "data_train_num_info.loc[data_train_num_info['descr'].isna(), \n",
    "                        'descr'] = data_train_num_info.loc[data_train_num_info['descr'].isna(), 'col']\n",
    "\n",
    "data_train_num_info = data_train_num_info.assign(feature_clear = data_train_num_info['descr'])\n",
    "\n",
    "expression = ' - стат.метрика'\n",
    "escaped_expression = re.escape(expression)\n",
    "\n",
    "data_train_num_info['feature_clear'] = data_train_num_info.feature_clear.apply(lambda x: re.sub(\\\n",
    "                                                                               escaped_expression, '', str(x)))\n",
    "\n",
    "data_train_num_info = data_train_num_info.drop('feature', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_to_use = list(data_train_num_info.loc[data_train_num_info['col'].isin(num_cols_to_use)].\\\n",
    "                                       groupby('feature_clear')['col'].max('target').values)\n",
    "\n",
    "len(num_cols_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_num = data_test[num_col_list]\n",
    "\n",
    "data_train_num_filtered = data_train_num[num_cols_to_use + ['target']]\n",
    "data_test_num_filtered = data_test_num[num_cols_to_use + ['target']]\n",
    "\n",
    "\n",
    "print(data_train_num_filtered.shape)\n",
    "print(data_test_num_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILTERING CATEGORICAL COLUMNS INTO SEPARATE DF \n",
    "\n",
    "data_train_cat = data_train[cat_col_list]\n",
    "\n",
    "data_test_cat = data_test[cat_col_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILTERING COLUMNS WITH BIGGEST TRAFFIC VAR INTO SEPARATE DF \n",
    "\n",
    "big_var_df_train = data_train_num_filtered[data_train_num_filtered['var_quant'] >= 8]\n",
    "big_var_df_test = data_test_num_filtered[data_test_num_filtered['var_quant'] >= 8]\n",
    "\n",
    "big_var_df_train = big_var_df_train.merge(data_train_cat, how  = 'left', left_index = True, right_index = True)\n",
    "big_var_df_test = big_var_df_test.merge(data_test_cat, how  = 'left', left_index = True, right_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILTERING COLUMNS WITH BIGGEST TRAFFIC VAR INTO SEPARATE DF \n",
    "\n",
    "small_var_df_train = data_train_num_filtered[data_train_num_filtered['var_quant'] < 8]\n",
    "small_var_df_test = data_test_num_filtered[data_test_num_filtered['var_quant'] < 8]\n",
    "\n",
    "small_var_df_train = small_var_df_train.merge(data_train_cat, how  = 'left', left_index = True, right_index = True)\n",
    "small_var_df_test = small_var_df_test.merge(data_test_cat, how  = 'left', left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALZING TARGET DIFFERENCE RATE FOR CATEGORICAL COLUMNS\n",
    "\n",
    "target_by_cat_cols = pd.DataFrame(index=cat_col_list, \n",
    "                                  columns=['0_bv', '1_bv', '1_to_0_rate_bv','0_sv', '1_sv', '1_to_0_rate_sv'])\n",
    "\n",
    "for col in cat_col_list:\n",
    "    temp = big_var_df_train.pivot_table(columns=col,values='target', aggfunc='median').fillna(0).applymap(float)\n",
    "#     print(temp)\n",
    "#     print(temp.values[0][1])\n",
    "    target_by_cat_cols.loc[col]['0_bv'] = temp[0].values[0]\n",
    "    try:\n",
    "        target_by_cat_cols.loc[col]['1_bv'] = temp[1].values[0]\n",
    "        target_by_cat_cols.loc[col]['1_to_0_rate_bv'] = temp[1].values[0]/temp[0].values[0]\n",
    "    except:\n",
    "        target_by_cat_cols.loc[col]['1_bv'] = np.nan\n",
    "        target_by_cat_cols.loc[col]['1_to_0_rate_bv'] = np.nan\n",
    "        \n",
    "    temp = small_var_df_train.pivot_table(columns=col,values='target', aggfunc='median').fillna(0).applymap(float)\n",
    "#     print(temp)\n",
    "#     print(temp.values[0][1])\n",
    "    target_by_cat_cols.loc[col]['0_sv'] = temp[0].values[0]\n",
    "    try:\n",
    "        target_by_cat_cols.loc[col]['1_sv'] = temp[1].values[0]\n",
    "        target_by_cat_cols.loc[col]['1_to_0_rate_sv'] = temp[1].values[0]/temp[0].values[0]\n",
    "    except:\n",
    "        target_by_cat_cols.loc[col]['1_sv'] = np.nan\n",
    "        target_by_cat_cols.loc[col]['1_to_0_rate_sv'] = np.nan\n",
    "\n",
    "\n",
    "ax1 = target_by_cat_cols.sort_index().loc[:, ['0_bv', '1_bv']].plot(kind = 'bar',\n",
    "                                                                       figsize = (10,5),\n",
    "                                                                       rot = 90,\n",
    "                                                                       sharex = True)\n",
    "\n",
    "\n",
    "ax1 = target_by_cat_cols.sort_index()['1_to_0_rate_bv'].plot(secondary_y=True,\n",
    "                                                                colormap = 'ocean',\n",
    "                                                                rot = 90, \n",
    "                                                                legend = True,\n",
    "                                                                sharex = True);\n",
    "\n",
    "ax1.set_title('big_var')\n",
    "    \n",
    "ax2 = target_by_cat_cols.sort_index().loc[:, ['0_sv', '1_sv']].plot(kind = 'bar',\n",
    "                                                                    figsize = (10,5),\n",
    "                                                                    rot = 90,\n",
    "                                                                    sharex = True);\n",
    "\n",
    "ax2 = target_by_cat_cols.sort_index()['1_to_0_rate_sv'].plot(secondary_y=True,\n",
    "                                                             colormap = 'ocean',\n",
    "                                                             rot = 90,\n",
    "                                                             legend = True,\n",
    "                                                             sharex = True);\n",
    "\n",
    "ax2.set_title('small_var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "X_train_big_var = big_var_df_train.fillna(0)\n",
    "\n",
    "plot_n_clusters = 20\n",
    "sse = []\n",
    "for k in range(1, plot_n_clusters+1):\n",
    "    \n",
    "        kmeans_batch = MiniBatchKMeans(n_clusters=k, \n",
    "                 init='k-means++', \n",
    "                 n_init=10, \n",
    "                 max_iter=300, \n",
    "                 random_state=1)\n",
    "        \n",
    "        kmeans_batch.fit(X_train_big_var)\n",
    "        sse.append(kmeans_batch.inertia_)\n",
    "        \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1, plot_n_clusters+1), sse)\n",
    "plt.xticks(range(1, plot_n_clusters+1))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Sum of squared error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "X_train_big_var = big_var_df_train.fillna(0).astype('float32')\n",
    "X_test_big_var = big_var_df_test.fillna(0).astype('float32')\n",
    "\n",
    "kmeans_batch = MiniBatchKMeans(n_clusters=4, \n",
    "            init='k-means++', \n",
    "            n_init=10,\n",
    "            max_iter=300, \n",
    "            tol=1e-04,\n",
    "            random_state=1)\n",
    "\n",
    "\n",
    "y_kmeans_batch_train = kmeans_batch.fit_predict(X_train_big_var)\n",
    "\n",
    "y_kmeans_batch_test = kmeans_batch.predict(X_test_big_var)\n",
    "\n",
    "\n",
    "big_var_df_train = big_var_df_train.assign(cluster = y_kmeans_batch_train)\n",
    "\n",
    "big_var_df_test = big_var_df_test.assign(cluster = y_kmeans_batch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING MULTICOLLENEARITY IN DATA\n",
    "\n",
    "def highlight_corr_06_plus(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_high = s >= 0.6\n",
    "    return ['background-color: red' if v else '' for v in is_high]\n",
    "\n",
    "collinear_cols_style = data_train_num_corr.loc[num_cols_to_use, num_cols_to_use].\\\n",
    "                                           style.apply(highlight_corr_06_plus).format(\"{:.2%}\")\n",
    "\n",
    "\n",
    "collinear_cols_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_num_corr.loc[num_cols_to_use, num_cols_to_use]['dpi_duration'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_num_corr.loc[num_cols_to_use, num_cols_to_use]['MV_ARPU'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_for_pca = ['MV_ARPU', 'MV_ap_4G_d', 'MV_AP_total','all_clc_std_mnt3', 'clc_no_vas_roam_std_mnt3']\n",
    "dpi_for_pca = ['dpi_duration', 'dpi_upload', 'dpi_download', 'dpi_events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_cols.sort()\n",
    "traffic_cols_plus_target = ['traff_m5', 'traff_m3','traff_m2','traff_m1', 'target']\n",
    "traffic_cols_plus_target.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIMETIONALITY REDUCTION BY PCA FOR MULTICOLLINEAR COLUMNS\n",
    "\n",
    "data_train_num_filtered = data_train_num_filtered.fillna(0)\n",
    "data_test_num_filtered = data_test_num_filtered.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x_traf = StandardScaler()\n",
    "\n",
    "X_train_std = sc_x_traf.fit_transform(data_train_num_filtered[totals_for_pca])\n",
    "X_test_std = sc_x_traf.fit_transform(data_test_num_filtered[totals_for_pca])\n",
    "\n",
    "pca_train = PCA(random_state=3)\n",
    "pca_test = PCA(random_state=3)\n",
    "\n",
    "X_train_pca_totals = pca_train.fit_transform(X_train_std)\n",
    "X_test_pca_totals = pca_test.fit_transform(X_train_std)\n",
    "\n",
    "pd.DataFrame(pca_train.explained_variance_ratio_).style.format(\"{:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x_traf = StandardScaler()\n",
    "\n",
    "X_train_std = sc_x_traf.fit_transform(data_train_num_filtered[dpi_for_pca])\n",
    "X_test_std = sc_x_traf.fit_transform(data_test_num_filtered[dpi_for_pca])\n",
    "\n",
    "pca_train = PCA(random_state=3)\n",
    "pca_test = PCA(random_state=3)\n",
    "\n",
    "X_train_pca_dpi = pca_train.fit_transform(X_train_std)\n",
    "X_test_pca_dpi = pca_test.fit_transform(X_train_std)\n",
    "\n",
    "pd.DataFrame(pca_train.explained_variance_ratio_).style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x_traf = StandardScaler()\n",
    "\n",
    "X_train_std = sc_x_traf.fit_transform(data_train_num_filtered[totals_for_pca])\n",
    "X_test_std = sc_x_traf.fit_transform(data_test_num_filtered[totals_for_pca])\n",
    "\n",
    "pca_train = PCA(n_components = 1, random_state=3)\n",
    "pca_test = PCA(n_components = 1, random_state=3)\n",
    "\n",
    "X_train_pca_totals = pca_train.fit_transform(X_train_std)\n",
    "X_test_pca_totals = pca_test.fit_transform(X_test_std)\n",
    "\n",
    "\n",
    "\n",
    "sc_x_traf = StandardScaler()\n",
    "\n",
    "X_train_std = sc_x_traf.fit_transform(data_train_num_filtered[dpi_for_pca])\n",
    "X_test_std = sc_x_traf.fit_transform(data_test_num_filtered[dpi_for_pca])\n",
    "\n",
    "pca_train = PCA(n_components = 2, random_state=3)\n",
    "pca_test = PCA(n_components = 2, random_state=3)\n",
    "\n",
    "X_train_pca_dpi = pca_train.fit_transform(X_train_std)\n",
    "X_test_pca_dpi = pca_test.fit_transform(X_test_std)\n",
    "\n",
    "\n",
    "del(X_train_std, X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_test_num, data_train_num, data_train_cat, data_test_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARING DATA FOR MODELS\n",
    "\n",
    "data_train_model = pd.read_parquet(dir_path + 'train/data_train.parquet.gzip').fillna(0)\n",
    "\n",
    "\n",
    "# data_train_model = data_train_num_filtered.drop(totals_for_pca, axis=1).fillna(0)\n",
    "# data_train_model = data_train_model.drop(dpi_for_pca, axis=1)\n",
    "# data_train_model = data_train_model.assign(pca1=X_train_pca_totals[:, 0],\n",
    "#                                            pca2=X_train_pca_dpi[:, 0],\n",
    "#                                            pca3=X_train_pca_dpi[:, 1])\n",
    "\n",
    "# data_train_model_varQ90 = data_train_model[data_train_model['var_quant'] >= 8]\n",
    "# data_train_model_varQ90 = data_train_model_varQ90.merge(big_var_df_train['cluster'], how='left',\n",
    "#                                                        left_index = True, right_index = True)\n",
    "\n",
    "# data_train_model_full = data_train_model\n",
    "# data_train_model = data_train_model[~data_train_model['var_quant'].isin([8,9])]\n",
    "\n",
    "# data_train_model_varQ90 = data_train_model_varQ90.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_model = pd.read_parquet(dir_path + 'test/data_test.parquet.gzip').fillna(0)\n",
    "\n",
    "# data_test_model = data_test_num_filtered.drop(totals_for_pca, axis=1).fillna(0)\n",
    "# data_test_model = data_test_model.drop(dpi_for_pca, axis=1)\n",
    "# data_test_model = data_test_model.assign(pca1=X_test_pca_totals[:, 0],\n",
    "#                                          pca2=X_test_pca_dpi[:, 0],\n",
    "#                                          pca3=X_test_pca_dpi[:, 1])\n",
    "\n",
    "# data_test_model_varQ90 = data_test_model[data_test_model['var_quant'] >= 8]\n",
    "# data_test_model_varQ90 = data_test_model_varQ90.merge(big_var_df_train['cluster'], how='left',\n",
    "#                                                       left_index = True, right_index = True)\n",
    "\n",
    "# data_test_model_full = data_test_model\n",
    "# data_test_model = data_test_model[~data_test_model['var_quant'].isin([8,9])]\n",
    "\n",
    "# data_test_model_varQ90 = data_test_model_varQ90.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_normalization(df, cols_to_exclude = []):\n",
    "    \n",
    "    \n",
    "    cols_to_use = list(df.columns)\n",
    "   \n",
    "    for col in cols_to_exclude:\n",
    "        cols_to_use.remove(col)\n",
    "        \n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    for column in cols_to_use: \n",
    "        df_norm[column] = (df_norm[column] - df_norm[column].min()) /(df_norm[column].max() - df_norm[column].min()) \n",
    "\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_train_model.loc[:, data_train_model.columns != 'target'], data_train_model.target\n",
    "\n",
    "# X = df_normalization(X, ['var_quant'])\n",
    "\n",
    "X = df_normalization(X, [])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state = 17)\n",
    "\n",
    "X_full, y_full = data_test_model_full.loc[:, data_test_model_full.columns != 'target'], data_test_model_full.target\n",
    "\n",
    "X_full = df_normalization(X_full, ['var_quant'])\n",
    "\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_full, y_full, test_size=0.30, random_state = 17)\n",
    "\n",
    "X_varQ90 = data_train_model_varQ90.loc[:, data_train_model_varQ90.columns != 'target']\n",
    "y_varQ90 = data_train_model_varQ90.target\n",
    "\n",
    "X_varQ90 = df_normalization(X_varQ90, ['var_quant'])\n",
    "\n",
    "X_train_varQ90, X_test_varQ90, y_train_varQ90, y_test_varQ90 = train_test_split(X_varQ90, y_varQ90, \n",
    "                                                                                test_size=0.30, random_state = 17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_test, y_test_test = data_test_model.loc[:, data_test_model.columns != 'target'], data_test_model.target\n",
    "\n",
    "# X_test_test = df_normalization(X_test_test, ['var_quant'])\n",
    "X_test_test = df_normalization(X_test_test, [])\n",
    "\n",
    "X_test_test_full = data_test_model_full.loc[:, data_test_model_full.columns != 'target']\n",
    "X_test_test_full = df_normalization(X_test_test_full, ['var_quant'])\n",
    "y_test_test_full = data_test_model_full.target\n",
    "\n",
    "X_test_test_varQ90 = data_test_model_varQ90.loc[:, data_test_model_varQ90.columns != 'target']\n",
    "X_test_test_varQ90 = df_normalization(X_test_test_varQ90, ['var_quant'])\n",
    "y_test__test_varQ90 = data_test_model_varQ90.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_model_varQ90.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION RESULTS\n",
    "\n",
    "\n",
    "# cols_traff_only = ['traff_m5', 'traff_m4', 'traff_m3', 'traff_m2', 'traff_m1']\n",
    "\n",
    "# Train set results\n",
    "# RMSE train: 8.863683, test: 8.775289\n",
    "# R^2 train: 0.420902, test: 0.428726\n",
    "\n",
    "# Test set results\n",
    "# RMSE test_test: 8.837257\n",
    "# R^2 test_test: 0.423239\n",
    "\n",
    "#ALL DATA NORMALIZED  - BETTER RESULTS, columns = chosen by best target correlation + var_quantiles + slope + cluster\n",
    "\n",
    "# FULL DATASET\n",
    "\n",
    "# Train set results\n",
    "# RMSE train: 8.749031, test: 8.664875\n",
    "# R^2 train: 0.435786, test: 0.443012\n",
    "\n",
    "# Test set results\n",
    "# RMSE test_test: 8.723869\n",
    "# R^2 test_test: 0.437945\n",
    "\n",
    "# DIFF MODEL BY QUANTILE \n",
    "# Train set results\n",
    "# RMSE train TOTAL: 8.503218, test TOTAL: 8.422132\n",
    "# R^2 train TOTAL: 0.486569, test TOTAL: 0.498329\n",
    "\n",
    "# Test set results\n",
    "# RMSE test test TOTAL: 8.730296\n",
    "# R^2 test test TOTAL: 0.437116\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL DATA SET\n",
    "\n",
    "slr = LinearRegression(normalize=True)\n",
    "\n",
    "slr.fit(X_train_full, y_train_full)\n",
    "\n",
    "y_train_pred = slr.predict(X_train_full)\n",
    "y_test_pred = slr.predict(X_test_full)\n",
    "\n",
    "print()\n",
    "print('Train set results')\n",
    "print()\n",
    "print('RMSE train: {:.6f}, test: {:.6f}'.format(\n",
    "        mean_squared_error(y_train_full, y_train_pred, squared = False),\n",
    "        mean_squared_error(y_test_full, y_test_pred, squared = False)))\n",
    "print('R^2 train: {:.6f}, test: {:.6f}'.format(\n",
    "        r2_score(y_train_full, y_train_pred),\n",
    "        r2_score(y_test_full, y_test_pred)))\n",
    "\n",
    "\n",
    "\n",
    "y_test_test_pred = slr.predict(X_test_test_full)\n",
    "\n",
    "print()\n",
    "print('Test set results')\n",
    "print()\n",
    "print('RMSE test_test: {:.6f}'.format(\n",
    "        mean_squared_error(y_test_test_full, y_test_test_pred, squared = False)))\n",
    "print('R^2 test_test: {:.6f}'.format(\n",
    "        r2_score(y_test_test_full, y_test_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "slr = LinearRegression(normalize=True)\n",
    "\n",
    "slr.fit(X_train_full, y_train_full)\n",
    "\n",
    "y_train_pred = slr.predict(X_train_full)\n",
    "y_test_pred = slr.predict(X_test_full)\n",
    "\n",
    "print()\n",
    "print('Train set results')\n",
    "print()\n",
    "print('RMSE train: {:.6f}, test: {:.6f}'.format(\n",
    "        mean_squared_error(y_train_full, y_train_pred, squared = False),\n",
    "        mean_squared_error(y_test_full, y_test_pred, squared = False)))\n",
    "print('R^2 train: {:.6f}, test: {:.6f}'.format(\n",
    "        r2_score(y_train_full, y_train_pred),\n",
    "        r2_score(y_test_full, y_test_pred)))\n",
    "\n",
    "\n",
    "\n",
    "y_test_test_pred = slr.predict(X_test_test_full)\n",
    "\n",
    "print()\n",
    "print('Test set results')\n",
    "print()\n",
    "print('RMSE test_test: {:.6f}'.format(\n",
    "        mean_squared_error(y_test_test_full, y_test_test_pred, squared = False)))\n",
    "print('R^2 test_test: {:.6f}'.format(\n",
    "        r2_score(y_test_test_full, y_test_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIFFERENT MODELS BY VAR_QUANT\n",
    "\n",
    "X_list, y_list = [], []\n",
    "df_list = []\n",
    "\n",
    "coeff = []\n",
    "\n",
    "for g, v in data_train_model.groupby('var_quant'):\n",
    "    \n",
    "    X_list.append(v.loc[:, v.columns != 'target'])\n",
    "    y_list.append(v.target)\n",
    "    \n",
    "#Adding data with 90th qauntile by traffic variance\n",
    "\n",
    "X_list.append(data_train_model_varQ90.loc[:, data_train_model_varQ90.columns != 'target'])\n",
    "y_list.append(data_train_model_varQ90.target)  \n",
    "\n",
    "\n",
    "for i, j in zip(X_list, y_list):\n",
    "    df_list.append(train_test_split(i, j, test_size=0.30, train_size = 0.70, random_state = 13))\n",
    "    \n",
    "\n",
    "y_train_pred, y_test_pred, y_train, y_test = pd.Series(), pd.Series(), pd.Series(), pd.Series()\n",
    "\n",
    "for X_tr, X_ts, y_tr, y_ts in df_list:\n",
    "    slr = LinearRegression(normalize=True)\n",
    "\n",
    "    slr.fit(X_tr, y_tr)\n",
    "    train_pred = pd.Series(data = slr.predict(X_tr), index=y_tr.index)\n",
    "    y_train_pred = y_train_pred.append(train_pred)\n",
    "#     print(y_train_pred.shape)\n",
    "    \n",
    "    test_pred = pd.Series(data = slr.predict(X_ts), index=y_ts.index)\n",
    "    y_test_pred = y_test_pred.append(test_pred)\n",
    "    \n",
    "    y_train = y_train.append(y_tr)\n",
    "    y_test = y_test.append(y_ts) \n",
    "    \n",
    "    print(str(X_tr['var_quant'].unique()[0])+' quantile set results')\n",
    "    print()\n",
    "\n",
    "    print('RMSE train: {:.6f}, test: {:.6f}'.format(\n",
    "        mean_squared_error(y_tr, train_pred, squared = False),\n",
    "        mean_squared_error(y_ts, test_pred, squared = False)))\n",
    "    print('R^2 train: {:.6f}, test: {:.6f}'.format(\n",
    "        r2_score(y_tr, train_pred),\n",
    "        r2_score(y_ts, test_pred)))\n",
    "    print()\n",
    "    \n",
    "    coeff.append(slr.coef_)    \n",
    "    \n",
    "    joblib_file = \"joblib_model\"+\"_q\"+str(int(X_tr['var_quant'].unique()[0]))+\".pkl\"\n",
    "    joblib.dump(slr, joblib_file)\n",
    "\n",
    "print('RMSE train TOTAL: {:.6f}, test TOTAL: {:.6f}'.format(\n",
    "mean_squared_error(y_train, y_train_pred, squared = False),\n",
    "mean_squared_error(y_test, y_test_pred, squared = False)))  \n",
    "\n",
    "print('R^2 train TOTAL: {:.6f}, test TOTAL: {:.6f}'.format(\n",
    "        r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIFFERENT MODELS BY VAR_QUANT TEST_TEST SET\n",
    "\n",
    "X_list, y_list = [], []\n",
    "df_list = []\n",
    "\n",
    "\n",
    "for g, v in data_test_model.groupby('var_quant'):\n",
    "    \n",
    "    X_list.append(v.loc[:, v.columns != 'target'])\n",
    "    y_list.append(v.target)\n",
    "    \n",
    "X_list.append(data_test_model_varQ90.loc[:, data_test_model_varQ90.columns != 'target'])\n",
    "y_list.append(data_test_model_varQ90.target) \n",
    "    \n",
    "for i, j in zip(X_list, y_list):\n",
    "    df_list.append([i,j])\n",
    "    \n",
    "y_test_test_pred, y_test_test = pd.Series(), pd.Series()\n",
    "\n",
    "for X_ts, y_ts in df_list:\n",
    "    \n",
    "    joblib_file = \"joblib_model\"+\"_q\"+str(X_ts['var_quant'].unique()[0])+\".pkl\"\n",
    "    try: \n",
    "        slr = joblib.load(joblib_file)\n",
    "    except:\n",
    "        joblib_file = \"joblib_model\"+\"_q\"+str(X_ts['var_quant'].unique()[0]+1)+\".pkl\"\n",
    "        slr = joblib.load(joblib_file)\n",
    "\n",
    "    test_test_pred = pd.Series(data = slr.predict(X_ts), index=y_ts.index)\n",
    "    y_test_test_pred = y_test_test_pred.append(test_test_pred)\n",
    "    \n",
    "    print()\n",
    "    print(str(X_ts['var_quant'].unique()[0])+' quantile set results')\n",
    "    print()\n",
    "\n",
    "    print('RMSE test test: {:.6f}'.format(\n",
    "        mean_squared_error(y_ts, test_test_pred, squared = False)))\n",
    "    print('R^2 test test: {:.6f}'.format(\n",
    "        r2_score(y_ts, test_test_pred)))\n",
    "   \n",
    "    y_test_test = y_test_test.append(y_ts) \n",
    "   \n",
    "print()\n",
    "print('RMSE test test TOTAL: {:.6f}'.format(mean_squared_error(y_test_test, y_test_test_pred, squared = False)))\n",
    "print('R^2 test test TOTAL: {:.6f}'.format(r2_score(y_test_test, y_test_test_pred)))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO RESULTS\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_y = StandardScaler()\n",
    "sc_x = StandardScaler()\n",
    "\n",
    "\n",
    "X_train_std = sc_x.fit_transform(X_train_full)\n",
    "X_test_std = sc_x.transform(X_test_full)\n",
    "X_test_test_std = sc_x.transform(X_test_test_full)\n",
    "y_train_std = sc_y.fit_transform(y_train_full[:, np.newaxis]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.0001)\n",
    "lasso.fit(X_train_std,y_train_std)\n",
    "y_train_pred_std = lasso.predict(X_train_std)\n",
    "y_test_pred_std = lasso.predict(X_test_std)\n",
    "y_test_test_pred_std = lasso.predict(X_test_test_std)\n",
    "\n",
    "print('RMSE train: {:.6f}, test: {:.6f}, test_test: {:.6f}'.format(\n",
    "        mean_squared_error(y_train_full, sc_y.inverse_transform(y_train_pred_std), squared = False),\n",
    "        mean_squared_error(y_test_full, sc_y.inverse_transform(y_test_pred_std), squared = False), \n",
    "        mean_squared_error(y_test_test_full, sc_y.inverse_transform(y_test_test_pred_std), squared = False)))\n",
    "print('R^2 train: {:.6f}, test: {:.6f}, test_test:{:.6f}'.format(\n",
    "        r2_score(y_train_full, sc_y.inverse_transform(y_train_pred_std)),\n",
    "        r2_score(y_test_full, sc_y.inverse_transform(y_test_pred_std)),\n",
    "        r2_score(y_test_test_full, sc_y.inverse_transform(y_test_test_pred_std))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM REGRESSOR RESULTS\n",
    "\n",
    "\n",
    "#ALL DATA NORMALIZED  - BETTER RESULTS, columns = chosen by best target correlation + var_quantiles + slope + cluster\n",
    "\n",
    "# FULL DATASET\n",
    "\n",
    "# RMSE train: 7.973927, test: 8.563232, test_test: 8.155191\n",
    "# R^2 train: 0.531328, test: 0.456003, test_test:0.508833\n",
    "\n",
    "# Test set results\n",
    "\n",
    "\n",
    "# DIFF MODEL BY QUANTILE \n",
    "# Train set results\n",
    "\n",
    "\n",
    "# Test set results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##hight corr target columns, stats chosen by highest correlation with target + pcas + var quant\n",
    "\n",
    "\n",
    "#'traff_m1', 'traff_m2', 'traff_m3', 'traff_m5', 'var_quant','dpi_day_cnt', 'content_cnt_mea_mnt3', 'data_3g_tv_cnt_std_mnt1',\n",
    "# 'all_cnt_mea_mnt3', 'all_clc_std_mnt3', 'data_3g_tar_vol_std_mnt3','non_accum_internet_vol_std_mnt3', 'gprs_tar_vol_std_mnt3',\n",
    "# 'MV_ap_innet_out_v', 'MV_Traf_2G_d_Mb', 'MV_Traf_3G_d_Mb','MV_Traf_4G_d_Mb', 'pca1', 'pca2', 'pca3'\n",
    "\n",
    "# FULL DATASET\n",
    "\n",
    "# RMSE train: 7.985060, test: 8.567211, test_test: 8.164066 \n",
    "# R^2 train: 0.530019, test: 0.455497, test_test:0.507764\n",
    "\n",
    "# FULL DATASET NORMALIZED DATA\n",
    "\n",
    "# RMSE train: 7.977355, test: 8.551193, test_test: 8.153748 \n",
    "# R^2 train: 0.530926, test: 0.457531, test_test:0.509007\n",
    "\n",
    "# model = ltb.LGBMRegressor(learning_rate = 0.15, \n",
    "#                           n_estimators =300, \n",
    "#                           reg_alpha = 0.05 ,\n",
    "#                           objective ='regression',\n",
    "#                           random_state = 0)\n",
    "\n",
    "\n",
    "# RMSE train: 6.895497, test: 8.698676, test_test: 7.482220 !!!!!!!!!!!!!\n",
    "# R^2 train: 0.649527, test: 0.438658, test_test:0.586551\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# split by var_quant < 8 and var_quant  8 and 9 + cat_cols for q8 and q9 \n",
    "\n",
    "# TEST DATA RESULT\n",
    "\n",
    "# var_q_less_than_8 set results\n",
    "\n",
    "# RMSE test: 4.919048\n",
    "# R^2 test: 0.387249\n",
    "# RMSE test TOTAL: 4.919048\n",
    "\n",
    "# var_q_8_and_9 set results\n",
    "\n",
    "# RMSE test: 16.737090\n",
    "# R^2 test: 0.266719\n",
    "# RMSE test TOTAL: 8.682401\n",
    "\n",
    "# RMSE test TOTAL: 8.682401 !!!\n",
    "# R^2 test TOTAL: 0.443276\n",
    "\n",
    "# split by var_quant < 8 and var_quant  8 and 9 WITHOUT cat_cols for q8 and q9 \n",
    "\n",
    "# var_q_less_than_8 set results\n",
    "\n",
    "# RMSE test: 4.919048\n",
    "# R^2 test: 0.387249\n",
    "# RMSE test TOTAL: 4.919048\n",
    "\n",
    "# var_q_8_and_9 set results\n",
    "\n",
    "# RMSE test: 16.833100\n",
    "# R^2 test: 0.258282\n",
    "# RMSE test TOTAL: 8.719444\n",
    "\n",
    "# RMSE test TOTAL: 8.719444\n",
    "# R^2 test TOTAL: 0.438515\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X_train_full.drop('var_quant', axis = 1)\n",
    "X_test_full = X_test_full.drop('var_quant', axis = 1)\n",
    "X_test_test_full = X_test_test_full.drop('var_quant', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ltb.LGBMRegressor(objective ='regression',\n",
    "                          random_state = 0)\n",
    "\n",
    "\n",
    "model.fit(X_train_full,y_train_full)\n",
    "y_train_pred = model.predict(X_train_full)\n",
    "y_test_pred = model.predict(X_test_full)\n",
    "y_test_test_pred = model.predict(X_test_test_full)\n",
    "\n",
    "print('RMSE train: {:.6f}, test: {:.6f}, test_test: {:.6f}'.format(\n",
    "        mean_squared_error(y_train_full, y_train_pred, squared = False),\n",
    "        mean_squared_error(y_test_full, y_test_pred, squared = False), \n",
    "        mean_squared_error(y_test_test_full, y_test_test_pred, squared = False)))\n",
    "print('R^2 train: {:.6f}, test: {:.6f}, test_test:{:.6f}'.format(\n",
    "        r2_score(y_train_full, y_train_pred),\n",
    "        r2_score(y_test_full, y_test_pred),\n",
    "        r2_score(y_test_test_full, y_test_test_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "scoring = make_scorer(mean_squared_error, squared = False,greater_is_better=False)\n",
    "\n",
    "kf = KFold(n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "parameters = {'learning_rate': [0.05, 0.15, 0.20],\n",
    "              'n_estimators': [100, 200, 300, 400],\n",
    "              'reg_alpha':[0.05, 0.7, 0.8]}\n",
    "\n",
    "\n",
    "lgbm = LGBMRegressor(silent=True, random_state= 3)\n",
    "\n",
    "grid_search = GridSearchCV(lgbm, parameters, scoring=scoring, n_jobs=-1, cv=kf, verbose = True)\n",
    "res = grid_search.fit(X_train_full, y_train_full)\n",
    "\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best score: {grid_search.best_score_}', '\\n') \n",
    "print('Best parameters set:')\n",
    "\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(f'\\t{param_name}:  {best_parameters[param_name]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.cv_results_.get('mean_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.cv_results_.get('params')[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ltb.LGBMRegressor(objective ='regression',\n",
    "                          random_state = 0,\n",
    "                          learning_rate = 0.2,\n",
    "                          n_estimators = 400, \n",
    "                          reg_alpha = 0.08)\n",
    "\n",
    "\n",
    "model.fit(X_train_full,y_train_full)\n",
    "y_train_pred = model.predict(X_train_full)\n",
    "y_test_pred = model.predict(X_test_full)\n",
    "y_test_test_pred = model.predict(X_test_test_full)\n",
    "\n",
    "print('RMSE train: {:.6f}, test: {:.6f}, test_test: {:.6f}'.format(\n",
    "        mean_squared_error(y_train_full, y_train_pred, squared = False),\n",
    "        mean_squared_error(y_test_full, y_test_pred, squared = False), \n",
    "        mean_squared_error(y_test_test_full, y_test_test_pred, squared = False)))\n",
    "print('R^2 train: {:.6f}, test: {:.6f}, test_test:{:.6f}'.format(\n",
    "        r2_score(y_train_full, y_train_pred),\n",
    "        r2_score(y_test_full, y_test_pred),\n",
    "        r2_score(y_test_test_full, y_test_test_pred)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 637.85,
   "position": {
    "height": "659.85px",
    "left": "1540px",
    "right": "20px",
    "top": "22px",
    "width": "356px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
